{"cells":[{"metadata":{"id":"rMXCK08Z89ax"},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"id":"mwGW6c-3zd30","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport re \nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport seaborn as sns\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nnltk.download('stopwords') \nfrom nltk.stem import WordNetLemmatizer\nimport numpy as np \nimport pandas as pd\nimport datetime as dt","execution_count":null,"outputs":[]},{"metadata":{"id":"KwNMI1PVUjBV"},"cell_type":"markdown","source":"# Data Understanding"},{"metadata":{"id":"b8mx41TmTuIb","trusted":true},"cell_type":"code","source":"# loading the dataset\ndata = pd.read_csv(\"../input/covid19-sentiments/COVID-19_Sentiments.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ZgYJCrmRULAp","outputId":"18a413bd-1b5b-4688-a148-7e15c98d7f86","trusted":true},"cell_type":"code","source":"## viewing the dataset\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"udTSXGaJURPi","outputId":"f38e2dce-0f6a-4400-dba9-092150498d81","trusted":true},"cell_type":"code","source":"# checking the bottom of the dataset\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"ImCEyOByUU5B","outputId":"9da8f127-4054-4d68-b6d4-1c8525844390","trusted":true},"cell_type":"code","source":"### the shape of the data\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"n3omJT_LUZlk","outputId":"2a7b3fa7-08d6-4106-fec1-e28c8df90de3","trusted":true},"cell_type":"code","source":"## data type\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"qx4Y-sKlVsf-"},"cell_type":"markdown","source":"# Data Cleaning and Manipulation"},{"metadata":{"id":"80cKhpVn97oZ"},"cell_type":"markdown","source":"**Duplicated entries**"},{"metadata":{"id":"AC1LkYeUV-VJ","outputId":"27f1c6d6-4ab5-4888-d74f-6cb9acf693f4","trusted":true},"cell_type":"code","source":"# check for duplicates\nif data.duplicated().any():\n    print('Duplicates Found')\nelse:    \n    print('No Duplicates Found')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing Dupicates\ndata = data.drop_duplicates().reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"qHo1Q_eeAkZE"},"cell_type":"markdown","source":"**Missing values**"},{"metadata":{"id":"4vHXIKIlWr5b","outputId":"3f98da54-7dd1-4593-935e-12df645d57a2","trusted":true},"cell_type":"code","source":"### missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"oem5CAA5Av4l"},"cell_type":"markdown","source":"**More data cleaning procedures**"},{"metadata":{"id":"rpEnCtSEdpTx","trusted":true},"cell_type":"code","source":"#imputing the missing sentiments\ndata['Sentiments'].fillna(data['Sentiments'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"edFb-vQOb_99","trusted":true},"cell_type":"code","source":"#changing the date to datetime\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# Create features for year, month, day\ndata['year'] = data['Date'].dt.year \ndata['month'] = data['Date'].dt.month \ndata['day'] = data['Date'].dt.day ","execution_count":null,"outputs":[]},{"metadata":{"id":"_agJfbyDechP","outputId":"7beced86-14b9-4b2f-c054-7cd795da5735","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"wg7JFZiwef4v","outputId":"d910db2e-ec53-4fb3-d594-b4f5f6ccd96d","trusted":true},"cell_type":"code","source":"#dropping columns that we do not need\ndata = data.drop([\"Date\", \"year\"], 1) #the year column has same value(2020)\n\nprint(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"BNE9UrwgWNfz"},"cell_type":"markdown","source":"**Reducing the sentiments to only 3 values**"},{"metadata":{"id":"BtnF6p6qWS74","outputId":"86c2c920-b4df-4053-91eb-cb61c80ddd36","trusted":true},"cell_type":"code","source":"# function to make the changes on the sentiment column\ndef classes_def(x):\n    if x < 0:\n        return \"Negative\"\n    elif x > 0:\n        return \"Positive\"\n    else:\n        return \"Neutral\"\n    \ndata['Sentiments']=data['Sentiments'].apply(lambda x:classes_def(x))\n\ntarget=data['Sentiments']","execution_count":null,"outputs":[]},{"metadata":{"id":"luXoYwxC0s-8","outputId":"9b6ded6f-b75d-4cc4-da77-9e1b7972d3cf","trusted":true},"cell_type":"code","source":"# checking the distribution of the sentiment column\ndata['Sentiments'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"guMkC1WeFt-6"},"cell_type":"markdown","source":">There is not a huge imbalance between the different classes. "},{"metadata":{"id":"vGJoS-g-Bjqn"},"cell_type":"markdown","source":"## Exploratory Data Analysis EDA"},{"metadata":{"id":"Hsq75bswXyKX"},"cell_type":"markdown","source":"### Count Plot of the sentiment classes"},{"metadata":{"id":"wMTjlczBAE8_","outputId":"99a0b098-ab07-4f56-da4f-91bec1cb3b4e","trusted":true},"cell_type":"code","source":"# Count plot of sentiments\nsns.countplot(data['Sentiments'], order = data['Sentiments'].value_counts().index)\nplt.title(\"Countplot of the sentiments\")\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count plot of sentiments\nsns.countplot(x='month', data = data)\nplt.title(\"No of Tweets in each month\")\n# plt.xlabel(['March','April','May'])\n# plt.xticks(rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"â‚¬\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n     \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","execution_count":null,"outputs":[]},{"metadata":{"id":"CEqdDxAJTTBY","outputId":"90e30903-0570-4008-beac-85c372cf38fb","trusted":true},"cell_type":"code","source":"# We will copy the text in another column so that the original text is also there for comparison\n\ndata['ctext'] = data.Text.astype(str)\ndef text_cleaner(text):\n    temp = text.lower()\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    temp = url_pattern.sub(r'', temp)\n    html=re.compile(r'<[^>]+>')\n    temp = html.sub(r'',temp)\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    temp = emoji_pattern.sub(r'', temp)\n    \n    table=str.maketrans('','',string.punctuation)\n    temp = temp.translate(table)\n    \n    tem=[]\n    for word in temp.split():\n        if word in abbreviations.keys():\n            tem.append(abbreviations[word])\n        else:\n            tem.append(word)\n    temp=tem\n    return (\" \".join(temp)).strip()\ndata['ctext'] = data['ctext'].apply(lambda text: text_cleaner(text))    \ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"6ZvmS9XoWGBS"},"cell_type":"markdown","source":"## **Number of characters in a tweet per sentiment**"},{"metadata":{"id":"2NSvZ0_CTk8Q","outputId":"0a335178-7571-4c4b-946b-cd39f7d9ecda","trusted":true},"cell_type":"code","source":"# defining the number of plots to make and figure size\nfig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\n\n# number of characters in the positive sentiments\ntweet_len_positive=data[data['Sentiments']==\"Positive\"]['ctext'].str.len()\nax1.hist(tweet_len_positive,color='#FACA0C')\nax1.set_title('Positive Sentiments')\n\n# number of characters in the negative sentiments\ntweet_len_negative=data[data['Sentiments']==\"Negative\"]['ctext'].str.len()\nax2.hist(tweet_len_negative,color='blue')\nax2.set_title('Negative Sentiments')\n\n# number of characters in the neutral sentiments\ntweet_len_neutral=data[data['Sentiments']==\"Neutral\"]['ctext'].str.len()\nax3.hist(tweet_len_neutral,color='purple')\nax3.set_title('Neutral Sentiments')\n\n\n\nfig.suptitle('Characters in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"QQ1biYQ2WLyD"},"cell_type":"markdown","source":"## **Number of words per tweets**"},{"metadata":{"id":"QyfuO3n9TlVb","outputId":"6a862cb1-c1d7-4ae6-e40f-5c7e9f4a1514","trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\n\ntweet_len_positive=data[data['Sentiments']==\"Positive\"]['ctext'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len_positive,color='yellow')\nax1.set_title('Positive Sentiments')\n\ntweet_len_negative=data[data['Sentiments']==\"Negative\"]['ctext'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len_negative,color='blue')\nax2.set_title('Negative Sentiments')\n\ntweet_len_neutral=data[data['Sentiments']==\"Neutral\"]['ctext'].str.split().map(lambda x: len(x))\nax3.hist(tweet_len_neutral,color='purple')\nax3.set_title('Neutral Sentiments')\n\nfig.suptitle('Words in a tweet')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"YsCABQcxXdLu"},"cell_type":"markdown","source":"## **Stop words**"},{"metadata":{"id":"lnNYuYMJTlrL","trusted":true},"cell_type":"code","source":"# function to check for stop words in each sentiment group\ndef create_corpus(target):\n  corpus=[]\n  \n  for x in data[data['Sentiments']==target ]['ctext'].str.split():\n    for i in x:\n      corpus.append(i)\n  \n  return corpus","execution_count":null,"outputs":[]},{"metadata":{"id":"FZyPMzPVJtMr","outputId":"65be684f-d10e-44cd-8c5d-2eb4da0bf33e","trusted":true},"cell_type":"code","source":"# initialize a variable to contain stopwords in English\nstop_words = set(stopwords.words('english')) \n\n# tokens of words \nword_tokens = data['ctext'].apply(nltk.word_tokenize) \n# word_tokens = word_tokenize(data['text']) \n    \nstop = [] \n  \nfor w in word_tokens: \n  for w in stop_words: \n    stop.append(w) ","execution_count":null,"outputs":[]},{"metadata":{"id":"9gUwOcamPMI7"},"cell_type":"markdown","source":"## **Common words**"},{"metadata":{"id":"VQFF1Q9kceHc"},"cell_type":"markdown","source":"Positive sentiments"},{"metadata":{"id":"bqZ2Rnn1H1Je","outputId":"a3264471-3f01-4465-f5bb-f71477e63343","trusted":true},"cell_type":"code","source":"from collections import defaultdict, Counter\n\n# checking for common words in the class \"Positive\"\ncorpus=create_corpus(\"Positive\")\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\n# for loop to append the word and count for most common words to empty lists\nfor word,count in most[1:100]:\n  # do not include stop words\n  if (word not in stop) :\n    x.append(word)\n    y.append(count)\n\n# plotting\nplt.figure(figsize=(18,10))\nplt.title(\"Common words in Positve Sentiments\")\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{"id":"VqrvV38gckPd"},"cell_type":"markdown","source":"Negative sentiments"},{"metadata":{"id":"YSdJHkgGSG1B","outputId":"5d52e9d7-e58e-4a10-ed00-5b72a4d1c52a","trusted":true},"cell_type":"code","source":"# checking for common words in the class \"Negative\"\ncorpus=create_corpus(\"Negative\")\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\n# for loop to append the word and count for most common words to empty lists\nfor word,count in most[1:100]:\n  # do not include stop words\n  if (word not in stop) :\n    x.append(word)\n    y.append(count)\n\n# plotting\nplt.figure(figsize=(18,10))\nplt.title(\"Common words in Negative Sentiments\")\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{"id":"sELWbV1rcu-b"},"cell_type":"markdown","source":"Neutral sentiments"},{"metadata":{"id":"1sHg9kl_TS63","outputId":"5c6a9ffa-47b8-4209-d8aa-603a270a12cd","trusted":true},"cell_type":"code","source":"# checking for common words in the class \"Neutral\"\ncorpus=create_corpus(\"Neutral\")\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\n# for loop to append the word and count for most common words to empty lists\nfor word,count in most[1:100]:\n  # do not include stop words\n  if (word not in stop) :\n    x.append(word)\n    y.append(count)\n\n# plotting\nplt.figure(figsize=(18,10))\nplt.title(\"Common words in Neutral Sentiments\")\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{"id":"AC__N_5ZcEeE"},"cell_type":"markdown","source":"## **Most polular Hashtags for the period**"},{"metadata":{"id":"ABOKIhOKKvRI","outputId":"8cc4755d-33c9-475f-d808-662f95493b4c","trusted":true},"cell_type":"code","source":"# function to find hashtags in the text\ndef find_hash(text):\n    line=re.findall(r'(?<=#)\\w+',text)\n    return \" \".join(line)\n\n# apply the function to the text\ndata['hash']=data['Text'].apply(lambda x:find_hash(x))\n\n# create a temporary dataframe that contains the value counts for the top 45 most popular hashtags\ntemp1=data['hash'].value_counts()[:][1:45]\ntemp1= temp1.to_frame().reset_index().rename(columns={'index':'Hashtag','hash':'count'})\n\n# make a plot of the popular hashtags\nplt.figure(figsize=(15,10))\n# sns.countplot(x='hash',data=data)\nsns.barplot(x=\"Hashtag\",y=\"count\", data = temp1)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"id":"S3kSo5LS9Idg"},"cell_type":"markdown","source":"## **Most popular mentions**"},{"metadata":{"id":"TMKZor0OJMKc","outputId":"b4b63dda-1c7d-4e21-d82c-880766a24035","trusted":true},"cell_type":"code","source":"# function to find mentions(@) in the text\ndef mentions(text):\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)\n\n # apply the function to the text   \ndata['mentions']=data['Text'].apply(lambda x:mentions(x))\n\n# create a temporary dataframe that contains the value counts for the top 70 most popular mentions\ntemp2=data['mentions'].value_counts()[:][1:70]\ntemp2 =temp2.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n\n# make a plot of the popular hashtags\nplt.figure(figsize=(20,15))\nsns.barplot(x=\"Mentions\",y=\"count\", data = temp2)\nplt.xticks(rotation=90)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"zDuWL0Je4C_D","trusted":true},"cell_type":"code","source":"# create a temporary dataframe that contains the hashtags \ntemp_hash=data['hash'].value_counts()\ntemp_hash= temp_hash.to_frame().reset_index().rename(columns={'index':'Hashtag','hash':'count', 'location':'location'})\n\n# create a temporary dataframe that contains the mentions \ntemp_men=data['mentions'].value_counts()\ntemp_men=temp_men.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count', 'location':'location'})\n\n# # export the dataframes as csv files for use to plot on tableau\n# hashtags = temp_hash.to_csv(\"h#shtags.csv\")\n# mentions = temp_men.to_csv(\"m@ntions.csv\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"_Rc6HbBG8HZG"},"cell_type":"markdown","source":"## **Word Clouds for each Sentiment**"},{"metadata":{"id":"p3OPJ2u880JY"},"cell_type":"markdown","source":"<font color='blue'>Positive sentiments"},{"metadata":{"id":"MaVdMMpNyyZh","outputId":"77dd7290-9bce-472a-8544-f0990e724c7b","trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud \ndata_pos = data[data[\"Sentiments\"]==\"Positive\"]\ndata_neg = data[data[\"Sentiments\"]==\"Negative\"]\ndata_neu = data[data[\"Sentiments\"]==\"Neutral\"]\n\ncomment_words = '' \nstopwords = set(stop_words) \n\nfor val in data_pos.ctext: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n\nwordcloud1 = WordCloud(width = 1000, height = 1000, \n                background_color ='black',\n                colormap=\"Blues\",\n                stopwords = stopwords, \n                min_font_size = 15).generate(comment_words) \nplt.figure(figsize=(8,10))\nax1 = plt.imshow(wordcloud1)\n# ax1.axis('off')\nplt.title('Positive Sentiments',fontsize=35);","execution_count":null,"outputs":[]},{"metadata":{"id":"sF-uhG2e9TFE"},"cell_type":"markdown","source":"<font color='blue'>Negative sentiments"},{"metadata":{"id":"62r_5ljGTksJ","outputId":"cf961b5a-9e5b-4a00-9d50-dba6bf267b69","trusted":true},"cell_type":"code","source":"comment_words = ''\n\nfor val in data_neg.ctext: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n\nwordcloud2 = WordCloud(width = 800, height = 800, \n                background_color ='slategrey',\n                colormap=\"Reds\",\n                stopwords = stopwords, \n                min_font_size = 15).generate(comment_words)  \nplt.figure(figsize=(8,10))\nax2=plt.imshow(wordcloud2)\n# ax2.axis('off')\nplt.title('Negative Sentiments',fontsize=35);\n","execution_count":null,"outputs":[]},{"metadata":{"id":"tJaAX0nV9jni"},"cell_type":"markdown","source":"<font color='blue'>Neutral sentiments"},{"metadata":{"id":"Hou_k5jy9dG2","outputId":"05c318d6-44ac-46a5-a389-38f8c683a0de","trusted":true},"cell_type":"code","source":"comment_words = ''\nfor val in data_neu.ctext: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n\nwordcloud3 = WordCloud(width = 800, height = 800, \n                background_color ='darkolivegreen',\n                colormap=\"Greys\",\n                stopwords = stopwords, \n                min_font_size = 15).generate(comment_words) \nplt.figure(figsize=(8,10))\nax3 = plt.imshow(wordcloud3)\n# ax3.axis('off')\nplt.title('Neutral Sentiments',fontsize=35);","execution_count":null,"outputs":[]},{"metadata":{"id":"Cd5TN9SGI79T"},"cell_type":"markdown","source":"# **Pre-processing**"},{"metadata":{"id":"0U67XK6PM7nX","outputId":"300ff0f5-121f-470c-8fcf-a15aacafd03f","trusted":true},"cell_type":"code","source":"import re\nimport nltk\nimport nltk\nnltk.download('wordnet')\nlemma = WordNetLemmatizer()\n\n\ndef cleaner(orig_tweet):\n    # remove urls/links\n    orig_tweet = re.sub(r'http\\S+', ' ', orig_tweet)\n    \n    # remove html tags\n    orig_tweet = re.sub(r'<.*?>',' ', orig_tweet)\n    \n    # remove digits\n    orig_tweet = re.sub(r'\\d+',' ', orig_tweet)\n\n    # remove hashtags\n    orig_tweet = re.sub(r'#\\w+',' ', orig_tweet)\n    \n    # remove mentions\n    orig_tweet = re.sub(r'@\\w+',' ', orig_tweet)\n\n    # Removing any punctuation\n    orig_tweet = re.sub(r'[^\\w\\s]', '', orig_tweet)\n\n     # converting all characters to lowercase\n    orig_tweet = str(orig_tweet).lower()\n\n    # tokenization\n    orig_tweet = word_tokenize(orig_tweet)\n    \n    #removing stop words\n    orig_tweet = [word for word in orig_tweet if word not in stop_words]\n    \n    # lemmatization\n    orig_tweet = [lemma.lemmatize(word=w, pos='v') for w in orig_tweet]\n\n    # remove words with length <=2\n    orig_tweet = [i for i in orig_tweet if len(i) > 2]\n\n    return orig_tweet\n","execution_count":null,"outputs":[]},{"metadata":{"id":"L0lcyJ9li3DN","trusted":true},"cell_type":"code","source":"# apply the function to the text column for easy comparison with the original tweet\ndata['ctext'] = data['ctext'].apply(cleaner)","execution_count":null,"outputs":[]},{"metadata":{"id":"uN9Qz2SGTmZm","outputId":"a5c5601b-d4a0-4d6c-961e-9eda0e4a68fd","trusted":true},"cell_type":"code","source":"# check the dataframe to see changes made\ndata.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"H5ghfYqvUhhJ","trusted":true},"cell_type":"code","source":"# Finally, we will transform the data into occurrences, \n# \nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# This converts the list of words into space-separated strings\ndata['ctext'] = data['ctext'].apply(lambda x: ' '.join(x))\n\ncount_vect = CountVectorizer()\ncounts = count_vect.fit_transform(data['ctext'])","execution_count":null,"outputs":[]},{"metadata":{"id":"M5efKH9uU_RD","trusted":true},"cell_type":"code","source":"# We could leave it as the simple word-count per message, but it is better to use Term Frequency Inverse Document Frequency, more known as tf-idf\n# \nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntransformer = TfidfTransformer().fit(counts)\n\ncounts = transformer.transform(counts)","execution_count":null,"outputs":[]},{"metadata":{"id":"c0lF_GJ5a9h6"},"cell_type":"markdown","source":"# **Modelling**"},{"metadata":{"id":"yg5BVEwvVtOu"},"cell_type":"markdown","source":"## Naive Bayes Model"},{"metadata":{"id":"vaR_DHimVBXH","outputId":"d6d9468d-e433-4ff9-cec7-3f76d7f81cd7","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(counts, data['Sentiments'], test_size=0.2, random_state=69)\n\nfrom sklearn.naive_bayes import MultinomialNB\n\n# modelling using the Multinomial Naive Bayes model\nMNB = MultinomialNB().fit(X_train, y_train)\n\n# predicting using test set\npredicted = MNB.predict(X_test)\nprint(np.mean(predicted == y_test)) #getting accuracy of the model","execution_count":null,"outputs":[]},{"metadata":{"id":"oiDtYYHMTndj"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"id":"O5GkJZKaTsSy","outputId":"50af6074-99d8-4f04-e77c-6ebc818fcf2f","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrandom=RandomForestClassifier(n_estimators=5,  random_state=82)\nrandom.fit(X_train, y_train)  ","execution_count":null,"outputs":[]},{"metadata":{"id":"VYf6hVTaVV-5","trusted":true},"cell_type":"code","source":"# Prediction \ny_pred = random.predict(X_test)\n\n# Evaluation of the model\nprint(np.mean(y_pred == y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bernolli NB"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB(binarize = 0.1)\n\nmodel = clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction \ny_pred = model.predict(X_test)\n\n# Evaluation of the model\nprint(np.mean(y_pred == y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"bMi44_OCTsx4"},"cell_type":"markdown","source":"SVM"},{"metadata":{"id":"ayVQiLGXTuWi","trusted":true},"cell_type":"code","source":"# Building the model \nfrom sklearn.svm import SVC\nrbf = SVC(kernel='rbf', gamma=4, C=10)\n# Training the model using the training set\nrbf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"CjGSZcipT-GU","trusted":true},"cell_type":"code","source":"# Prediction \ny_pred = rbf.predict(X_test)\n\n# Evaluation of the model\nprint(np.mean(y_pred == y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"cjIO0TOsWBVb"},"cell_type":"markdown","source":"## Neural Networks"},{"metadata":{"id":"QqWXacbAT0Hy"},"cell_type":"markdown","source":"TensorFlow"},{"metadata":{"id":"PLwY2A6C3upG"},"cell_type":"markdown","source":"The best model for predicting sentiments according to their accuracy is Bernoulli with an accuracy of \n81.9% "},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\ndef text_cleaner(text):\n    temp = text.lower()\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    temp = url_pattern.sub(r'', temp)\n    html=re.compile(r'<[^>]+>')\n    temp = html.sub(r'',temp)\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    temp = emoji_pattern.sub(r'', temp)\n    \n    table=str.maketrans('','',string.punctuation)\n    temp = temp.translate(table)\n    \n    tem=[]\n    for word in temp.split():\n        if word in abbreviations.keys():\n            tem.append(abbreviations[word])\n        else:\n            tem.append(word)\n    temp=tem\n    \n    stop_words = set(stopwords.words('english'))\n    le=WordNetLemmatizer()\n    tokens = [le.lemmatize(word) for word in temp if not word in stop_words]\n    \n    long_words=[]\n    for i in tokens:\n        if len(i)>1:                                            \n            long_words.append(i)   \n    return (\" \".join(long_words)).strip()","execution_count":null,"outputs":[]},{"metadata":{"id":"Bl0gDU7IZdrb","trusted":true},"cell_type":"code","source":"X_features = data['Text'].copy()\nX_cleaned=X_features.apply(text_cleaner)\ntk = Tokenizer()\ntk.fit_on_texts(X_cleaned)\nX = tk.texts_to_sequences(X_cleaned)\nX = pad_sequences(X, padding=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dic = {'Positive':1,'Neutral':0,'Negative':2}\nY = data['Sentiments'].apply(lambda x : dic[x]) ","execution_count":null,"outputs":[]},{"metadata":{"id":"1NouOmQcaBFx","outputId":"9d89267a-c9b3-43da-f853-06263f3f354b","trusted":true},"cell_type":"code","source":"vocab_size = len(tk.word_index)+1\ntf.keras.backend.clear_session()\n# hyper parameters\nEPOCHS = 2\nBATCH_SIZE = 1024\nembedding_dim = 16\nunits = 256\n\nmodel = tf.keras.Sequential([\n    L.Embedding(vocab_size, embedding_dim, input_length=X.shape[1]),\n    L.Bidirectional(L.LSTM(units,return_sequences=True)),\n    L.GlobalMaxPool1D(),\n    L.Dropout(0.4),\n    L.Dense(3,activation = 'softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X,Y,verbose=1,batch_size = 2048,validation_split=0.2,epochs = 5)\nmodel.save('LSTM.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}